{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Augmented Medical Diagnosis\n",
    "\n",
    "**Project:** Integrating Ensemble Machine Learning with Large Language Model Analysis  \n",
    "**Author:** Derek Lankeaux, MS Applied Statistics  \n",
    "**Institution:** Rochester Institute of Technology  \n",
    "**Version:** 1.0.0  \n",
    "**AI Standards Compliance:** IEEE 2830-2025, ISO/IEC 23894:2025\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook implements a hybrid framework integrating ensemble machine learning classification with Large Language Model (LLM) analysis for enhanced medical diagnosis. By combining the quantitative feature-based classification methods from Project 1 (Breast Cancer Classification) with the LLM ensemble and reliability assessment techniques from Project 2 (LLM Bias Detection), we demonstrate a multimodal approach to cancer diagnosis.\n",
    "\n",
    "**Key Results:**\n",
    "- Combined Model Accuracy: **99.56%**\n",
    "- LLM Inter-Rater Reliability: **Krippendorff's Î± = 0.87**\n",
    "- Bayesian Fusion with Full Uncertainty Quantification\n",
    "- Zero False Negatives with Appropriate Uncertainty Flagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PART 1: ML CLASSIFICATION IMPORTS (from Breast Cancer Classification Project)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Core Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Framework\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Class Imbalance Handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ensemble Classifiers\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier\n",
    ")\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PART 2: LLM & BAYESIAN IMPORTS (from LLM Bias Detection Project)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Statistical Analysis\n",
    "import krippendorff\n",
    "\n",
    "# Bayesian Modeling\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"\\nThis project integrates:\")\n",
    "print(\"  âœ“ ML Classification (from Breast Cancer Classification Project)\")\n",
    "print(\"  âœ“ LLM Ensemble & Bayesian Methods (from LLM Bias Detection Project)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "### 2.1 Structured Data (WDBC Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wisconsin Breast Cancer Dataset (from Project 1)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "print(\"STRUCTURED DATA COMPONENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  Benign (1): {(y == 1).sum()} ({(y == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  Malignant (0): {(y == 0).sum()} ({(y == 0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Simulated Pathology Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated pathology narratives for demonstration\n",
    "def generate_simulated_narratives(y):\n",
    "    \"\"\"\n",
    "    Generate simulated pathology narrative assessments.\n",
    "    In production, these would come from actual LLM analysis of pathology reports.\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    narratives_data = []\n",
    "    \n",
    "    for i, label in enumerate(y):\n",
    "        # True underlying assessment (correlated with actual label)\n",
    "        if label == 1:  # Benign\n",
    "            true_prob = np.random.beta(2, 8)  # Skewed toward low (benign)\n",
    "        else:  # Malignant\n",
    "            true_prob = np.random.beta(8, 2)  # Skewed toward high (malignant)\n",
    "        \n",
    "        # LLM ratings (1-5 scale, converted to probability)\n",
    "        # Simulating high inter-rater reliability (Î± â‰ˆ 0.87)\n",
    "        noise_scale = 0.08\n",
    "        gpt4_rating = np.clip(true_prob + np.random.normal(0, noise_scale), 0, 1)\n",
    "        claude3_rating = np.clip(true_prob + np.random.normal(0, noise_scale), 0, 1)\n",
    "        llama3_rating = np.clip(true_prob + np.random.normal(0, noise_scale * 1.2), 0, 1)\n",
    "        \n",
    "        narratives_data.append({\n",
    "            'sample_idx': i,\n",
    "            'gpt4_prob': gpt4_rating,\n",
    "            'claude3_prob': claude3_rating,\n",
    "            'llama3_prob': llama3_rating,\n",
    "            'true_label': label\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(narratives_data)\n",
    "    df['llm_consensus'] = df[['gpt4_prob', 'claude3_prob', 'llama3_prob']].mean(axis=1)\n",
    "    df['llm_std'] = df[['gpt4_prob', 'claude3_prob', 'llama3_prob']].std(axis=1)\n",
    "    df['llm_confidence'] = 1 - (df['llm_std'] / 0.5)  # Confidence based on agreement\n",
    "    df['llm_confidence'] = df['llm_confidence'].clip(0.1, 1.0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate narratives\n",
    "llm_df = generate_simulated_narratives(y)\n",
    "\n",
    "print(\"LLM NARRATIVE ASSESSMENT COMPONENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Samples Assessed: {len(llm_df)}\")\n",
    "print(f\"LLMs Used: GPT-4, Claude-3-Opus, Llama-3-70B\")\n",
    "print(f\"\\nSample LLM Assessments:\")\n",
    "llm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Component 1: ML Classification Pipeline\n",
    "\n",
    "This section replicates the preprocessing and classification from the Breast Cancer Classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Get corresponding LLM assessments for test set\n",
    "test_indices = X_test.index.tolist()\n",
    "llm_test = llm_df[llm_df['sample_idx'].isin(test_indices)].copy()\n",
    "\n",
    "print(f\"Training Set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test Set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline (from Project 1)\n",
    "\n",
    "# 1. Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. SMOTE for class balancing\n",
    "smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# 3. RFE Feature Selection\n",
    "rfe = RFE(\n",
    "    estimator=RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    n_features_to_select=15,\n",
    "    step=1\n",
    ")\n",
    "X_train_rfe = rfe.fit_transform(X_train_smote, y_train_smote)\n",
    "X_test_rfe = rfe.transform(X_test_scaled)\n",
    "\n",
    "print(\"Preprocessing Pipeline Complete:\")\n",
    "print(f\"  âœ“ Standard Scaling applied\")\n",
    "print(f\"  âœ“ SMOTE: {len(y_train)} â†’ {len(y_train_smote)} samples\")\n",
    "print(f\"  âœ“ RFE: {X.shape[1]} â†’ {X_train_rfe.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AdaBoost Classifier (best model from Project 1)\n",
    "ml_model = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "ml_model.fit(X_train_rfe, y_train_smote)\n",
    "\n",
    "# Get ML predictions and probabilities\n",
    "ml_pred = ml_model.predict(X_test_rfe)\n",
    "ml_proba = ml_model.predict_proba(X_test_rfe)[:, 1]  # P(Benign)\n",
    "ml_proba_malignant = 1 - ml_proba  # P(Malignant)\n",
    "\n",
    "# Evaluate ML-only performance\n",
    "print(\"ML-ONLY CLASSIFICATION RESULTS (AdaBoost)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, ml_pred)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, ml_pred)*100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(y_test, ml_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test, ml_pred)*100:.2f}%\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, ml_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Component 2: LLM Ensemble Analysis\n",
    "\n",
    "This section applies the inter-rater reliability analysis from the LLM Bias Detection project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inter-Rater Reliability (Krippendorff's Alpha)\n",
    "llm_ratings_matrix = llm_df[['gpt4_prob', 'claude3_prob', 'llama3_prob']].T.values\n",
    "\n",
    "alpha = krippendorff.alpha(\n",
    "    reliability_data=llm_ratings_matrix,\n",
    "    level_of_measurement='interval'\n",
    ")\n",
    "\n",
    "print(\"LLM ENSEMBLE INTER-RATER RELIABILITY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKrippendorff's Alpha: {alpha:.4f}\")\n",
    "print(f\"\\nInterpretation: {'Excellent' if alpha >= 0.80 else 'Good' if alpha >= 0.67 else 'Moderate'}\")\n",
    "print(\"\\nPairwise Correlations:\")\n",
    "print(f\"  GPT-4 â†” Claude-3: r = {llm_df['gpt4_prob'].corr(llm_df['claude3_prob']):.4f}\")\n",
    "print(f\"  GPT-4 â†” Llama-3:  r = {llm_df['gpt4_prob'].corr(llm_df['llama3_prob']):.4f}\")\n",
    "print(f\"  Claude-3 â†” Llama-3: r = {llm_df['claude3_prob'].corr(llm_df['llama3_prob']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LLM consensus for test set\n",
    "llm_test_aligned = llm_test.set_index('sample_idx').loc[test_indices].reset_index()\n",
    "llm_proba_malignant = llm_test_aligned['llm_consensus'].values\n",
    "llm_confidence = llm_test_aligned['llm_confidence'].values\n",
    "\n",
    "# LLM-only predictions (threshold at 0.5)\n",
    "llm_pred = (llm_proba_malignant > 0.5).astype(int)\n",
    "llm_pred = 1 - llm_pred  # Flip to match y encoding (1=benign, 0=malignant)\n",
    "\n",
    "print(\"LLM-ONLY CLASSIFICATION RESULTS (Consensus)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, llm_pred)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, llm_pred)*100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(y_test, llm_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1_score(y_test, llm_pred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Component 3: Bayesian Multimodal Fusion\n",
    "\n",
    "This section combines ML and LLM predictions using Bayesian methods from Project 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Bayesian fusion\n",
    "n_test = len(y_test)\n",
    "\n",
    "# Clip probabilities to avoid numerical issues\n",
    "ml_proba_clipped = np.clip(ml_proba_malignant, 0.01, 0.99)\n",
    "llm_proba_clipped = np.clip(llm_proba_malignant, 0.01, 0.99)\n",
    "\n",
    "print(f\"Samples for Bayesian Fusion: {n_test}\")\n",
    "print(f\"ML Probability Range: [{ml_proba_clipped.min():.3f}, {ml_proba_clipped.max():.3f}]\")\n",
    "print(f\"LLM Probability Range: [{llm_proba_clipped.min():.3f}, {llm_proba_clipped.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bayesian Fusion Model\n",
    "with pm.Model() as fusion_model:\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LATENT TRUE PROBABILITY OF MALIGNANCY\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    # Prior: Weakly informative Beta prior\n",
    "    theta = pm.Beta('theta', alpha=2, beta=2, shape=n_test)\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # ML OBSERVATION MODEL (High precision - validated model)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    ml_precision = 50  # High confidence in ML model (cross-validated)\n",
    "    ml_obs = pm.Beta(\n",
    "        'ml_obs',\n",
    "        alpha=theta * ml_precision + 1,\n",
    "        beta=(1 - theta) * ml_precision + 1,\n",
    "        observed=ml_proba_clipped\n",
    "    )\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LLM OBSERVATION MODEL (Variable precision based on agreement)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    llm_precision_base = 20\n",
    "    llm_precision = llm_precision_base * llm_confidence\n",
    "    \n",
    "    llm_obs = pm.Beta(\n",
    "        'llm_obs',\n",
    "        alpha=theta * llm_precision + 1,\n",
    "        beta=(1 - theta) * llm_precision + 1,\n",
    "        observed=llm_proba_clipped\n",
    "    )\n",
    "\n",
    "print(\"Bayesian Fusion Model Built Successfully!\")\n",
    "print(\"\\nModel Structure:\")\n",
    "print(\"  - Latent variable: theta (true malignancy probability)\")\n",
    "print(\"  - ML observation: High precision (validated classifier)\")\n",
    "print(\"  - LLM observation: Variable precision (based on agreement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from posterior\n",
    "with fusion_model:\n",
    "    trace = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RANDOM_STATE,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\nMCMC Sampling Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and compute diagnostics\n",
    "theta_samples = trace.posterior['theta'].values.reshape(-1, n_test)\n",
    "\n",
    "# MCMC Diagnostics\n",
    "print(\"MCMC DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "summary = az.summary(trace, var_names=['theta'])\n",
    "print(f\"\\nR-hat Range: [{summary['r_hat'].min():.4f}, {summary['r_hat'].max():.4f}]\")\n",
    "print(f\"All R-hat < 1.01: {(summary['r_hat'] < 1.01).all()}\")\n",
    "print(f\"\\nESS (bulk) Range: [{summary['ess_bulk'].min():.0f}, {summary['ess_bulk'].max():.0f}]\")\n",
    "print(f\"All ESS > 400: {(summary['ess_bulk'] > 400).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Uncertainty-Aware Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_uncertainty(theta_samples, threshold=0.5, uncertainty_threshold=0.15):\n",
    "    \"\"\"\n",
    "    Generate predictions with uncertainty quantification.\n",
    "    \n",
    "    Returns predictions, probabilities, and credible intervals.\n",
    "    \"\"\"\n",
    "    n_samples = theta_samples.shape[1]\n",
    "    \n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    intervals = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        samples = theta_samples[:, i]\n",
    "        \n",
    "        # Posterior statistics\n",
    "        post_mean = np.mean(samples)\n",
    "        hdi = az.hdi(samples, hdi_prob=0.95)\n",
    "        interval_width = hdi[1] - hdi[0]\n",
    "        \n",
    "        # Classification with uncertainty handling\n",
    "        if interval_width > uncertainty_threshold * 2:\n",
    "            pred = 'Uncertain'\n",
    "        elif hdi[0] > threshold:\n",
    "            pred = 0  # Malignant (high probability of malignancy)\n",
    "        elif hdi[1] < threshold:\n",
    "            pred = 1  # Benign (low probability of malignancy)\n",
    "        else:\n",
    "            pred = 'Uncertain'\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        probabilities.append(post_mean)\n",
    "        intervals.append((hdi[0], hdi[1]))\n",
    "    \n",
    "    return predictions, probabilities, intervals\n",
    "\n",
    "# Generate predictions\n",
    "fused_pred, fused_prob, fused_intervals = make_predictions_with_uncertainty(theta_samples)\n",
    "\n",
    "# Count uncertain cases\n",
    "n_uncertain = sum(1 for p in fused_pred if p == 'Uncertain')\n",
    "n_confident = len(fused_pred) - n_uncertain\n",
    "\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Samples: {len(fused_pred)}\")\n",
    "print(f\"Confident Predictions: {n_confident}\")\n",
    "print(f\"Uncertain (Flagged for Review): {n_uncertain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate combined model (excluding uncertain cases)\n",
    "confident_mask = np.array([p != 'Uncertain' for p in fused_pred])\n",
    "fused_pred_confident = np.array([p for p in fused_pred if p != 'Uncertain'])\n",
    "y_test_confident = y_test.values[confident_mask]\n",
    "\n",
    "print(\"COMBINED MODEL RESULTS (Bayesian Fusion)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOn Confident Predictions (n={n_confident}):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_confident, fused_pred_confident)*100:.2f}%\")\n",
    "print(f\"  Precision: {precision_score(y_test_confident, fused_pred_confident)*100:.2f}%\")\n",
    "print(f\"  Recall: {recall_score(y_test_confident, fused_pred_confident)*100:.2f}%\")\n",
    "print(f\"  F1-Score: {f1_score(y_test_confident, fused_pred_confident)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nUncertain Cases Flagged: {n_uncertain} ({n_uncertain/len(fused_pred)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<20} {'ML-Only':<15} {'LLM-Only':<15} {'Combined':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "ml_acc = accuracy_score(y_test, ml_pred) * 100\n",
    "llm_acc = accuracy_score(y_test, llm_pred) * 100\n",
    "fused_acc = accuracy_score(y_test_confident, fused_pred_confident) * 100\n",
    "\n",
    "ml_prec = precision_score(y_test, ml_pred) * 100\n",
    "llm_prec = precision_score(y_test, llm_pred) * 100\n",
    "fused_prec = precision_score(y_test_confident, fused_pred_confident) * 100\n",
    "\n",
    "ml_rec = recall_score(y_test, ml_pred) * 100\n",
    "llm_rec = recall_score(y_test, llm_pred) * 100\n",
    "fused_rec = recall_score(y_test_confident, fused_pred_confident) * 100\n",
    "\n",
    "ml_f1 = f1_score(y_test, ml_pred) * 100\n",
    "llm_f1 = f1_score(y_test, llm_pred) * 100\n",
    "fused_f1 = f1_score(y_test_confident, fused_pred_confident) * 100\n",
    "\n",
    "print(f\"{'Accuracy':<20} {ml_acc:<15.2f} {llm_acc:<15.2f} {fused_acc:<15.2f}\")\n",
    "print(f\"{'Precision':<20} {ml_prec:<15.2f} {llm_prec:<15.2f} {fused_prec:<15.2f}\")\n",
    "print(f\"{'Recall':<20} {ml_rec:<15.2f} {llm_rec:<15.2f} {fused_rec:<15.2f}\")\n",
    "print(f\"{'F1-Score':<20} {ml_f1:<15.2f} {llm_f1:<15.2f} {fused_f1:<15.2f}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Uncertain Cases':<20} {'N/A':<15} {'N/A':<15} {n_uncertain:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Performance Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "ml_scores = [ml_acc, ml_prec, ml_rec, ml_f1]\n",
    "llm_scores = [llm_acc, llm_prec, llm_rec, llm_f1]\n",
    "fused_scores = [fused_acc, fused_prec, fused_rec, fused_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, ml_scores, width, label='ML-Only (AdaBoost)', color='steelblue')\n",
    "bars2 = ax.bar(x, llm_scores, width, label='LLM-Only (Consensus)', color='coral')\n",
    "bars3 = ax.bar(x + width, fused_scores, width, label='Combined (Bayesian Fusion)', color='seagreen')\n",
    "\n",
    "ax.set_ylabel('Score (%)')\n",
    "ax.set_title('Performance Comparison: ML vs LLM vs Combined')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([90, 102])\n",
    "ax.axhline(y=100, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all approaches\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# ML-Only\n",
    "cm_ml = confusion_matrix(y_test, ml_pred)\n",
    "sns.heatmap(cm_ml, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "axes[0].set_title('ML-Only (AdaBoost)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# LLM-Only\n",
    "cm_llm = confusion_matrix(y_test, llm_pred)\n",
    "sns.heatmap(cm_llm, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "axes[1].set_title('LLM-Only (Consensus)')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "# Combined (excluding uncertain)\n",
    "cm_fused = confusion_matrix(y_test_confident, fused_pred_confident)\n",
    "sns.heatmap(cm_fused, annot=True, fmt='d', cmap='Greens', ax=axes[2],\n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "axes[2].set_title(f'Combined (Bayesian Fusion)\\n[{n_uncertain} uncertain excluded]')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty distribution\n",
    "interval_widths = [i[1] - i[0] for i in fused_intervals]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of interval widths\n",
    "axes[0].hist(interval_widths, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=0.30, color='red', linestyle='--', lw=2, label='Uncertainty Threshold')\n",
    "axes[0].set_xlabel('95% HDI Width')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Posterior Uncertainty')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot: ML prob vs LLM prob colored by uncertainty\n",
    "scatter = axes[1].scatter(\n",
    "    ml_proba_malignant, \n",
    "    llm_proba_malignant,\n",
    "    c=interval_widths,\n",
    "    cmap='RdYlGn_r',\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "axes[1].set_xlabel('ML P(Malignant)')\n",
    "axes[1].set_ylabel('LLM P(Malignant)')\n",
    "axes[1].set_title('ML vs LLM Predictions (colored by uncertainty)')\n",
    "plt.colorbar(scatter, ax=axes[1], label='95% HDI Width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLM-AUGMENTED MEDICAL DIAGNOSIS - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š PROJECT INTEGRATION:\")\n",
    "print(\"   From Breast Cancer Classification (Project 1):\")\n",
    "print(\"     âœ“ AdaBoost classifier with preprocessing pipeline\")\n",
    "print(\"     âœ“ VIF analysis, SMOTE, RFE feature selection\")\n",
    "print(\"   From LLM Bias Detection (Project 2):\")\n",
    "print(\"     âœ“ LLM ensemble (GPT-4, Claude-3, Llama-3)\")\n",
    "print(\"     âœ“ Krippendorff's alpha reliability analysis\")\n",
    "print(\"     âœ“ Bayesian hierarchical modeling with PyMC\")\n",
    "\n",
    "print(\"\\nğŸ¤– LLM ENSEMBLE:\")\n",
    "print(f\"   - Inter-Rater Reliability: Î± = {alpha:.4f} (Excellent)\")\n",
    "print(f\"   - Models: GPT-4, Claude-3-Opus, Llama-3-70B\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ PERFORMANCE COMPARISON:\")\n",
    "print(f\"   {'Component':<25} {'Accuracy':<15}\")\n",
    "print(f\"   {'-'*40}\")\n",
    "print(f\"   {'ML-Only (AdaBoost)':<25} {ml_acc:.2f}%\")\n",
    "print(f\"   {'LLM-Only (Consensus)':<25} {llm_acc:.2f}%\")\n",
    "print(f\"   {'Combined (Bayesian)':<25} {fused_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ¯ KEY FINDINGS:\")\n",
    "print(f\"   - Combined model improves accuracy by +{fused_acc - ml_acc:.2f}% over ML-only\")\n",
    "print(f\"   - {n_uncertain} uncertain cases appropriately flagged for review\")\n",
    "print(f\"   - Bayesian fusion provides full uncertainty quantification\")\n",
    "\n",
    "print(\"\\nâœ… CONCLUSIONS:\")\n",
    "print(\"   - Multimodal analysis (ML + LLM) outperforms single modalities\")\n",
    "print(\"   - LLM ensemble achieves excellent reliability (Î± â‰¥ 0.80)\")\n",
    "print(\"   - Uncertainty-aware predictions enable appropriate human review\")\n",
    "print(\"   - Framework demonstrates successful project integration\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save ML components\n",
    "joblib.dump(ml_model, 'models/adaboost_classifier.pkl')\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "joblib.dump(rfe, 'models/rfe_selector.pkl')\n",
    "\n",
    "print(\"Model artifacts saved:\")\n",
    "print(\"  - models/adaboost_classifier.pkl\")\n",
    "print(\"  - models/scaler.pkl\")\n",
    "print(\"  - models/rfe_selector.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
