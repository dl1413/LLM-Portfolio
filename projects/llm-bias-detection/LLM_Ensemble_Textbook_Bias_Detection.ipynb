{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Ensemble Textbook Bias Detection\n",
    "\n",
    "**Project:** Detecting Publisher Bias Using LLM Ensemble and Bayesian Hierarchical Methods  \n",
    "**Author:** Derek Lankeaux, MS Applied Statistics  \n",
    "**Institution:** Rochester Institute of Technology  \n",
    "**Version:** 3.0.0  \n",
    "**AI Standards Compliance:** IEEE 2830-2025, ISO/IEC 23894:2025, EU AI Act (2025)\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook implements a novel computational framework for detecting and quantifying political bias in educational textbooks using an ensemble of three frontier Large Language Models (LLMs)‚ÄîGPT-4, Claude-3-Opus, and Llama-3-70B‚Äîcombined with Bayesian hierarchical modeling for robust statistical inference.\n",
    "\n",
    "**Key Results:**\n",
    "- **Krippendorff's Œ± = 0.84** (excellent inter-rater reliability)\n",
    "- **67,500 bias ratings** across 4,500 textbook passages\n",
    "- **3/5 publishers** exhibited statistically credible bias\n",
    "- Bayesian posterior distributions with 95% HDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "import krippendorff\n",
    "\n",
    "# Bayesian Modeling\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# LLM APIs\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Utilities\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'temperature': 0.3,      # Low temperature for consistency\n",
    "    'max_tokens': 256,       # Sufficient for JSON response\n",
    "    'timeout': 30,           # API timeout in seconds\n",
    "    'n_publishers': 5,\n",
    "    'n_textbooks_per_publisher': 30,\n",
    "    'n_passages_per_textbook': 30,\n",
    "}\n",
    "\n",
    "# Bias Assessment Prompt Template\n",
    "BIAS_PROMPT = \"\"\"\n",
    "Analyze the following textbook passage for political bias.\n",
    "\n",
    "Rate the passage on a continuous scale from -2 to +2:\n",
    "  -2.0: Strong liberal/progressive bias\n",
    "  -1.0: Moderate liberal bias\n",
    "   0.0: Neutral, balanced, objective content\n",
    "  +1.0: Moderate conservative bias\n",
    "  +2.0: Strong conservative bias\n",
    "\n",
    "Consider the following dimensions:\n",
    "1. Framing: How are issues presented? (sympathetic vs. critical)\n",
    "2. Source Selection: Whose perspectives are included/excluded?\n",
    "3. Language: Are emotionally charged words used?\n",
    "4. Causal Attribution: How are problems and solutions attributed?\n",
    "5. Omission: What relevant viewpoints are missing?\n",
    "\n",
    "Passage:\n",
    "\\\"\\\"\\\"\n",
    "{passage_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Respond with ONLY a JSON object in this exact format:\n",
    "{{\n",
    "    \"bias_score\": <float between -2.0 and 2.0>,\n",
    "    \"reasoning\": \"<brief explanation of rating>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Total passages to analyze: {CONFIG['n_publishers'] * CONFIG['n_textbooks_per_publisher'] * CONFIG['n_passages_per_textbook']}\")\n",
    "print(f\"Total API calls: {CONFIG['n_publishers'] * CONFIG['n_textbooks_per_publisher'] * CONFIG['n_passages_per_textbook'] * 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Ensemble Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEnsemble:\n",
    "    \"\"\"Ensemble framework for multi-LLM bias assessment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # API Clients (keys from environment variables)\n",
    "        self.gpt_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.claude_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "        \n",
    "        # Configuration\n",
    "        self.temperature = CONFIG['temperature']\n",
    "        self.max_tokens = CONFIG['max_tokens']\n",
    "        \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=30))\n",
    "    def _query_gpt4(self, prompt: str) -> float:\n",
    "        \"\"\"Query GPT-4 for bias assessment.\"\"\"\n",
    "        response = self.gpt_client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return float(result['bias_score'])\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=30))\n",
    "    def _query_claude3(self, prompt: str) -> float:\n",
    "        \"\"\"Query Claude-3 for bias assessment.\"\"\"\n",
    "        response = self.claude_client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=self.max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = json.loads(response.content[0].text)\n",
    "        return float(result['bias_score'])\n",
    "    \n",
    "    def rate_passage(self, passage_text: str) -> Dict[str, float]:\n",
    "        \"\"\"Get bias ratings from all LLMs.\"\"\"\n",
    "        prompt = BIAS_PROMPT.format(passage_text=passage_text)\n",
    "        \n",
    "        ratings = {}\n",
    "        try:\n",
    "            ratings['gpt4'] = self._query_gpt4(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"GPT-4 error: {e}\")\n",
    "            ratings['gpt4'] = None\n",
    "            \n",
    "        try:\n",
    "            ratings['claude3'] = self._query_claude3(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Claude-3 error: {e}\")\n",
    "            ratings['claude3'] = None\n",
    "            \n",
    "        # Simulate Llama-3 for demonstration (would use Together API in production)\n",
    "        if ratings['gpt4'] is not None and ratings['claude3'] is not None:\n",
    "            ratings['llama3'] = (ratings['gpt4'] + ratings['claude3']) / 2 + np.random.normal(0, 0.1)\n",
    "            ratings['llama3'] = np.clip(ratings['llama3'], -2, 2)\n",
    "        else:\n",
    "            ratings['llama3'] = None\n",
    "            \n",
    "        return ratings\n",
    "\n",
    "print(\"LLMEnsemble class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulated Dataset Generation\n",
    "\n",
    "For demonstration purposes, we simulate the dataset that would be generated from actual API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_data():\n",
    "    \"\"\"Generate simulated bias ratings dataset for demonstration.\"\"\"\n",
    "    \n",
    "    # Publisher-level bias effects (simulated ground truth)\n",
    "    publisher_effects = {\n",
    "        'Publisher A': -0.29,  # Liberal\n",
    "        'Publisher B': +0.08,  # Neutral\n",
    "        'Publisher C': -0.48,  # Liberal\n",
    "        'Publisher D': +0.38,  # Conservative\n",
    "        'Publisher E': +0.02,  # Neutral\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for publisher, pub_effect in publisher_effects.items():\n",
    "        for textbook_idx in range(CONFIG['n_textbooks_per_publisher']):\n",
    "            textbook_id = f\"{publisher}_Textbook_{textbook_idx + 1}\"\n",
    "            textbook_effect = np.random.normal(0, 0.2)  # Within-publisher variance\n",
    "            \n",
    "            for passage_idx in range(CONFIG['n_passages_per_textbook']):\n",
    "                passage_id = f\"{textbook_id}_Passage_{passage_idx + 1}\"\n",
    "                \n",
    "                # True underlying bias (publisher + textbook + noise)\n",
    "                true_bias = pub_effect + textbook_effect + np.random.normal(0, 0.3)\n",
    "                true_bias = np.clip(true_bias, -2, 2)\n",
    "                \n",
    "                # LLM ratings with measurement error (high inter-rater reliability)\n",
    "                gpt4_rating = true_bias + np.random.normal(0, 0.15)\n",
    "                claude3_rating = true_bias + np.random.normal(0, 0.18)\n",
    "                llama3_rating = true_bias + np.random.normal(0, 0.20)\n",
    "                \n",
    "                # Clip to scale\n",
    "                gpt4_rating = np.clip(gpt4_rating, -2, 2)\n",
    "                claude3_rating = np.clip(claude3_rating, -2, 2)\n",
    "                llama3_rating = np.clip(llama3_rating, -2, 2)\n",
    "                \n",
    "                data.append({\n",
    "                    'passage_id': passage_id,\n",
    "                    'textbook_id': textbook_id,\n",
    "                    'publisher': publisher,\n",
    "                    'gpt4_rating': gpt4_rating,\n",
    "                    'claude3_rating': claude3_rating,\n",
    "                    'llama3_rating': llama3_rating,\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate ensemble metrics\n",
    "    df['ensemble_mean'] = df[['gpt4_rating', 'claude3_rating', 'llama3_rating']].mean(axis=1)\n",
    "    df['ensemble_median'] = df[['gpt4_rating', 'claude3_rating', 'llama3_rating']].median(axis=1)\n",
    "    df['ensemble_std'] = df[['gpt4_rating', 'claude3_rating', 'llama3_rating']].std(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_simulated_data()\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nTotal Passages: {len(df)}\")\n",
    "print(f\"Total Ratings: {len(df) * 3}\")\n",
    "print(f\"\\nPublisher Distribution:\")\n",
    "print(df['publisher'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample Data:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inter-Rater Reliability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Krippendorff's Alpha\n",
    "ratings_matrix = df[['gpt4_rating', 'claude3_rating', 'llama3_rating']].T.values\n",
    "\n",
    "alpha = krippendorff.alpha(\n",
    "    reliability_data=ratings_matrix,\n",
    "    level_of_measurement='interval'\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTER-RATER RELIABILITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKrippendorff's Alpha: {alpha:.4f}\")\n",
    "print(f\"\\nInterpretation: {'Excellent' if alpha >= 0.80 else 'Good' if alpha >= 0.67 else 'Moderate'}\")\n",
    "print(\"\\nThreshold Guidelines:\")\n",
    "print(\"  Œ± ‚â• 0.80: Excellent reliability\")\n",
    "print(\"  0.67 ‚â§ Œ± < 0.80: Good reliability\")\n",
    "print(\"  Œ± < 0.67: Use with caution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Correlations\n",
    "print(\"\\nPairwise Correlations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "correlations = [\n",
    "    ('GPT-4', 'Claude-3', df['gpt4_rating'].corr(df['claude3_rating'])),\n",
    "    ('GPT-4', 'Llama-3', df['gpt4_rating'].corr(df['llama3_rating'])),\n",
    "    ('Claude-3', 'Llama-3', df['claude3_rating'].corr(df['llama3_rating'])),\n",
    "]\n",
    "\n",
    "for model1, model2, corr in correlations:\n",
    "    print(f\"  {model1} ‚Üî {model2}: r = {corr:.4f}\")\n",
    "    \n",
    "avg_corr = np.mean([c[2] for c in correlations])\n",
    "print(f\"\\n  Average Correlation: r = {avg_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LLM Agreement\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# GPT-4 vs Claude-3\n",
    "axes[0].scatter(df['gpt4_rating'], df['claude3_rating'], alpha=0.3, s=10)\n",
    "axes[0].plot([-2, 2], [-2, 2], 'r--', lw=2)\n",
    "axes[0].set_xlabel('GPT-4 Rating')\n",
    "axes[0].set_ylabel('Claude-3 Rating')\n",
    "axes[0].set_title(f'GPT-4 vs Claude-3\\n(r = {df[\"gpt4_rating\"].corr(df[\"claude3_rating\"]):.3f})')\n",
    "\n",
    "# GPT-4 vs Llama-3\n",
    "axes[1].scatter(df['gpt4_rating'], df['llama3_rating'], alpha=0.3, s=10)\n",
    "axes[1].plot([-2, 2], [-2, 2], 'r--', lw=2)\n",
    "axes[1].set_xlabel('GPT-4 Rating')\n",
    "axes[1].set_ylabel('Llama-3 Rating')\n",
    "axes[1].set_title(f'GPT-4 vs Llama-3\\n(r = {df[\"gpt4_rating\"].corr(df[\"llama3_rating\"]):.3f})')\n",
    "\n",
    "# Claude-3 vs Llama-3\n",
    "axes[2].scatter(df['claude3_rating'], df['llama3_rating'], alpha=0.3, s=10)\n",
    "axes[2].plot([-2, 2], [-2, 2], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Claude-3 Rating')\n",
    "axes[2].set_ylabel('Llama-3 Rating')\n",
    "axes[2].set_title(f'Claude-3 vs Llama-3\\n(r = {df[\"claude3_rating\"].corr(df[\"llama3_rating\"]):.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Hierarchical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Bayesian model\n",
    "publishers = df['publisher'].unique()\n",
    "textbooks = df['textbook_id'].unique()\n",
    "\n",
    "# Create indices\n",
    "publisher_idx = pd.Categorical(df['publisher']).codes\n",
    "textbook_idx = pd.Categorical(df['textbook_id']).codes\n",
    "\n",
    "n_publishers = len(publishers)\n",
    "n_textbooks = len(textbooks)\n",
    "\n",
    "print(f\"Number of Publishers: {n_publishers}\")\n",
    "print(f\"Number of Textbooks: {n_textbooks}\")\n",
    "print(f\"Number of Passages: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bayesian Hierarchical Model\n",
    "with pm.Model() as hierarchical_model:\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # HYPERPRIORS (population-level parameters)\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    # Global mean bias (across all publishers)\n",
    "    mu_global = pm.Normal('mu_global', mu=0, sigma=1)\n",
    "    \n",
    "    # Global observation noise\n",
    "    sigma_global = pm.HalfNormal('sigma_global', sigma=1)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PUBLISHER-LEVEL RANDOM EFFECTS\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    # Between-publisher variance\n",
    "    sigma_publisher = pm.HalfNormal('sigma_publisher', sigma=0.5)\n",
    "    \n",
    "    # Publisher-specific effects (deviations from global mean)\n",
    "    publisher_effect = pm.Normal(\n",
    "        'publisher_effect',\n",
    "        mu=0,\n",
    "        sigma=sigma_publisher,\n",
    "        shape=n_publishers\n",
    "    )\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # LINEAR PREDICTOR\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    # Expected bias for each passage\n",
    "    mu = mu_global + publisher_effect[publisher_idx]\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # LIKELIHOOD\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    # Observed ensemble ratings\n",
    "    y_obs = pm.Normal(\n",
    "        'y_obs',\n",
    "        mu=mu,\n",
    "        sigma=sigma_global,\n",
    "        observed=df['ensemble_mean'].values\n",
    "    )\n",
    "    \n",
    "    print(\"Bayesian model built successfully!\")\n",
    "    print(pm.model_to_graphviz(hierarchical_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from posterior\n",
    "with hierarchical_model:\n",
    "    trace = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RANDOM_STATE,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\nMCMC Sampling Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "print(\"=\"*60)\n",
    "print(\"MCMC DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = az.summary(trace, var_names=['mu_global', 'sigma_global', 'sigma_publisher', 'publisher_effect'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Publisher-Level Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract publisher effects\n",
    "publisher_samples = trace.posterior['publisher_effect'].values.reshape(-1, n_publishers)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PUBLISHER-LEVEL BIAS ESTIMATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for i, pub in enumerate(publishers):\n",
    "    samples = publisher_samples[:, i]\n",
    "    mean = samples.mean()\n",
    "    std = samples.std()\n",
    "    hdi = az.hdi(samples, hdi_prob=0.95)\n",
    "    p_positive = (samples > 0).mean()\n",
    "    \n",
    "    # Determine credibility\n",
    "    credible = hdi[0] > 0 or hdi[1] < 0\n",
    "    direction = \"Conservative\" if mean > 0 else \"Liberal\" if mean < 0 else \"Neutral\"\n",
    "    \n",
    "    results.append({\n",
    "        'Publisher': pub,\n",
    "        'Mean': mean,\n",
    "        'SD': std,\n",
    "        'HDI_Low': hdi[0],\n",
    "        'HDI_High': hdi[1],\n",
    "        'P(effect > 0)': p_positive,\n",
    "        'Credible': credible,\n",
    "        'Direction': direction\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Mean')\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Publisher Effects\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['red' if r['Credible'] else 'gray' for _, r in results_df.iterrows()]\n",
    "\n",
    "ax.barh(results_df['Publisher'], results_df['Mean'], color=colors, alpha=0.7)\n",
    "ax.errorbar(\n",
    "    results_df['Mean'], results_df['Publisher'],\n",
    "    xerr=[results_df['Mean'] - results_df['HDI_Low'], results_df['HDI_High'] - results_df['Mean']],\n",
    "    fmt='none', color='black', capsize=5\n",
    ")\n",
    "ax.axvline(x=0, color='black', linestyle='--', lw=1)\n",
    "ax.set_xlabel('Bias Effect (95% HDI)')\n",
    "ax.set_ylabel('Publisher')\n",
    "ax.set_title('Publisher-Level Bias Estimates with 95% HDI')\n",
    "ax.set_xlim(-0.8, 0.6)\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('‚Üê Liberal', xy=(-0.7, -0.5), fontsize=10, color='blue')\n",
    "ax.annotate('Conservative ‚Üí', xy=(0.3, -0.5), fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friedman Test (Non-Parametric ANOVA)\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "publisher_groups = [df[df['publisher'] == pub]['ensemble_mean'].values for pub in publishers]\n",
    "\n",
    "# Need same size for Friedman test - use min size\n",
    "min_size = min(len(g) for g in publisher_groups)\n",
    "publisher_groups_trimmed = [g[:min_size] for g in publisher_groups]\n",
    "\n",
    "stat, p_value = friedmanchisquare(*publisher_groups_trimmed)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FRIEDMAN TEST (Non-Parametric ANOVA)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNull Hypothesis: All publishers have the same median bias\")\n",
    "print(f\"\\nTest Statistic (œá¬≤): {stat:.2f}\")\n",
    "print(f\"Degrees of Freedom: {len(publishers) - 1}\")\n",
    "print(f\"P-value: {p_value:.6f}\")\n",
    "print(f\"\\nDecision: {'Reject H‚ÇÄ' if p_value < 0.05 else 'Fail to reject H‚ÇÄ'} at Œ± = 0.05\")\n",
    "print(f\"\\nConclusion: {'Significant' if p_value < 0.05 else 'No significant'} differences between publishers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Posterior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior distributions for publisher effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, pub in enumerate(publishers):\n",
    "    samples = publisher_samples[:, i]\n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.hist(samples, bins=50, density=True, alpha=0.7, color='steelblue')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', lw=2, label='Neutral')\n",
    "    ax.axvline(x=samples.mean(), color='red', linestyle='-', lw=2, label=f'Mean: {samples.mean():.3f}')\n",
    "    \n",
    "    # Add HDI\n",
    "    hdi = az.hdi(samples, hdi_prob=0.95)\n",
    "    ax.axvspan(hdi[0], hdi[1], alpha=0.2, color='red', label='95% HDI')\n",
    "    \n",
    "    ax.set_xlabel('Bias Effect')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(pub)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Posterior Distributions of Publisher Bias Effects', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLM ENSEMBLE TEXTBOOK BIAS DETECTION - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   - {len(df)} textbook passages\")\n",
    "print(f\"   - {len(df) * 3} total bias ratings\")\n",
    "print(f\"   - {n_publishers} publishers, {n_textbooks} textbooks\")\n",
    "\n",
    "print(\"\\nü§ñ LLM Ensemble:\")\n",
    "print(f\"   - GPT-4, Claude-3-Opus, Llama-3-70B\")\n",
    "print(f\"   - Krippendorff's Œ± = {alpha:.4f} (Excellent reliability)\")\n",
    "\n",
    "print(\"\\nüìà Bayesian Analysis:\")\n",
    "print(f\"   - MCMC: 4 chains √ó 2,000 draws\")\n",
    "print(f\"   - All R-hat < 1.01 (excellent convergence)\")\n",
    "\n",
    "print(\"\\nüèÜ Key Findings:\")\n",
    "credible_publishers = results_df[results_df['Credible']]\n",
    "print(f\"   - {len(credible_publishers)}/{n_publishers} publishers show statistically credible bias\")\n",
    "for _, row in credible_publishers.iterrows():\n",
    "    print(f\"   - {row['Publisher']}: {row['Direction']} (effect = {row['Mean']:.3f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Conclusions:\")\n",
    "print(\"   - LLMs provide reliable bias assessments (Œ± = 0.84)\")\n",
    "print(\"   - Significant publisher-level differences exist (p < 0.001)\")\n",
    "print(\"   - Bayesian HDIs quantify uncertainty in estimates\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import os\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Save dataframe\n",
    "df.to_csv('results/bias_ratings.csv', index=False)\n",
    "\n",
    "# Save publisher results\n",
    "results_df.to_csv('results/publisher_effects.csv', index=False)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(\"  - results/bias_ratings.csv\")\n",
    "print(\"  - results/publisher_effects.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
