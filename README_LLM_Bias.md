# Detecting Publisher Bias Using LLM Ensemble and Bayesian Methods

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Reliability](https://img.shields.io/badge/Krippendorff's%20Œ±-0.84-success.svg)]()
[![LLMs](https://img.shields.io/badge/LLM%20Models-3-informational.svg)]()

## üî¨ Project Overview

This project presents a **novel framework** for detecting political bias in educational textbooks using an ensemble of three frontier Large Language Models (LLMs) combined with Bayesian hierarchical modeling. By analyzing **150 textbooks** (4,500 passages, 67,500 total ratings), the system quantifies publisher-level bias with rigorous statistical validation.

**Author:** Derek Lankeaux  
**Institution:** Rochester Institute of Technology  
**Program:** MS Applied Statistics  

---

## ‚ú® Key Features

### ü§ñ LLM Ensemble Architecture
- **GPT-4** (OpenAI) ‚Äî Industry-leading frontier model
- **Claude-3-Opus** (Anthropic) ‚Äî Constitutional AI with nuanced reasoning
- **Llama-3-70B** (Meta) ‚Äî Open-source frontier model

### üìä Statistical Rigor
- ‚úÖ **Krippendorff's Œ± = 0.84** ‚Äî Excellent inter-rater reliability
- ‚úÖ **Bayesian Hierarchical Modeling** ‚Äî Publisher + textbook random effects
- ‚úÖ **PyMC Implementation** ‚Äî MCMC sampling with convergence diagnostics
- ‚úÖ **Credible Intervals** ‚Äî 95% HDI for uncertainty quantification
- ‚úÖ **Hypothesis Testing** ‚Äî Friedman test + Wilcoxon pairwise comparisons

### üéØ Research Impact
- **67,500 API calls** processed (~2.5M tokens analyzed)
- **Statistically significant** publisher differences detected (p < 0.01)
- **Production-ready pipeline** for continuous textbook evaluation
- **Open-source framework** adaptable to any bias detection task

---

## üìÇ Repository Structure

```
textbook-bias-detection/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ LLM_Ensemble_Textbook_Bias_Detection.ipynb  # Main analysis
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ llm_ensemble.py           # LLM API wrapper classes
‚îÇ   ‚îú‚îÄ‚îÄ bayesian_model.py         # PyMC hierarchical model
‚îÇ   ‚îú‚îÄ‚îÄ statistical_tests.py      # Hypothesis testing functions
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Helper functions
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ textbook_passages.csv     # Dataset (if publicly available)
‚îÇ   ‚îî‚îÄ‚îÄ publisher_metadata.json   # Publisher information
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ posterior_samples/        # MCMC traces
‚îÇ   ‚îú‚îÄ‚îÄ visualizations/           # Generated plots
‚îÇ   ‚îî‚îÄ‚îÄ statistical_reports/      # Summary tables
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### Prerequisites
- Python 3.8 or higher
- API keys for OpenAI, Anthropic, Together AI (for LLM access)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/dereklankeaux/textbook-bias-detection.git
cd textbook-bias-detection
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Set up API keys:**
```bash
export OPENAI_API_KEY='your-openai-key'
export ANTHROPIC_API_KEY='your-anthropic-key'
export TOGETHER_API_KEY='your-together-key'
```

4. **Run the analysis:**
```bash
jupyter notebook notebooks/LLM_Ensemble_Textbook_Bias_Detection.ipynb
```

### Dependencies
```
pandas>=1.3.0
numpy>=1.21.0
pymc>=5.0.0
arviz>=0.14.0
scipy>=1.7.0
krippendorff>=0.5.0
openai>=1.0.0
anthropic>=0.8.0
together>=0.2.0
matplotlib>=3.4.0
seaborn>=0.11.0
tqdm>=4.62.0
jupyter>=1.0.0
```

---

## üìä Dataset

**Textbook Corpus Structure:**

- **Publishers:** 5 major educational publishers
- **Textbooks:** 30 per publisher (150 total)
- **Passages:** 30 per textbook (4,500 total)
- **Ratings:** 3 LLMs √ó 4,500 passages = **67,500 total evaluations**

**Bias Rating Scale:**
- **-2.0:** Strong liberal bias
- **-1.0:** Moderate liberal bias
- **0.0:** Neutral/balanced
- **+1.0:** Moderate conservative bias
- **+2.0:** Strong conservative bias

**Topics Analyzed:**
- Political systems & governance
- Economic policy
- Social issues
- Historical events
- Environmental policy

---

## üî¨ Methodology

### 1. LLM Ensemble Framework

```python
class LLMEnsemble:
    """Coordinate multiple LLM bias assessments"""
    
    def __init__(self):
        self.models = ['GPT-4', 'Claude-3-Opus', 'Llama-3-70B']
        # Initialize API clients
        
    def rate_passage(self, passage_text: str) -> Dict[str, float]:
        """Get bias ratings from all three LLMs"""
        prompt = f"""Analyze the following textbook passage for political bias.
        Rate on scale from -2 (strong liberal) to +2 (strong conservative).
        0 indicates neutral content.
        
        Passage: {passage_text}
        
        Respond with JSON: {{"bias_score": <number>, "reasoning": "<explanation>"}}
        """
        
        return {
            'gpt4': self.query_gpt4(prompt),
            'claude3': self.query_claude3(prompt),
            'llama3': self.query_llama3(prompt)
        }
```

### 2. Inter-Rater Reliability

**Krippendorff's Alpha Calculation:**
```python
import krippendorff

# Ratings matrix: (n_raters=3, n_units=4500)
ratings_matrix = df[['gpt4_rating', 'claude3_rating', 'llama3_rating']].T.values

# Calculate alpha (interval metric)
alpha = krippendorff.alpha(reliability_data=ratings_matrix, 
                           level_of_measurement='interval')

# Result: Œ± = 0.84 (excellent agreement)
```

**Interpretation:**
- **Œ± ‚â• 0.80:** Excellent reliability ‚úÖ
- **0.67 ‚â§ Œ± < 0.80:** Good reliability
- **0.60 ‚â§ Œ± < 0.67:** Moderate reliability
- **Œ± < 0.60:** Poor reliability

### 3. Bayesian Hierarchical Model

**Model Specification:**
```python
with pm.Model() as hierarchical_model:
    # Global parameters
    mu_global = pm.Normal('mu_global', mu=0, sigma=1)
    sigma_global = pm.HalfNormal('sigma_global', sigma=1)
    
    # Publisher-level random effects
    sigma_publisher = pm.HalfNormal('sigma_publisher', sigma=0.5)
    publisher_effect = pm.Normal('publisher_effect', mu=0, 
                                 sigma=sigma_publisher, shape=n_publishers)
    
    # Textbook-level random effects (nested)
    sigma_textbook = pm.HalfNormal('sigma_textbook', sigma=0.3)
    textbook_effect = pm.Normal('textbook_effect', mu=0, 
                                sigma=sigma_textbook, shape=n_textbooks)
    
    # Linear predictor
    mu = mu_global + publisher_effect[publisher_idx] + textbook_effect[textbook_idx]
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma_global, observed=ensemble_ratings)
    
    # MCMC sampling
    trace = pm.sample(2000, tune=1000, target_accept=0.95)
```

**Model Advantages:**
- **Partial pooling:** Balances publisher-specific estimates with global mean
- **Uncertainty quantification:** Full posterior distributions, not just point estimates
- **Hierarchical structure:** Accounts for nested variability (textbooks within publishers)
- **Regularization:** Prevents overfitting to individual publishers

---

## üìà Results

### Inter-Rater Reliability

| Metric | GPT-4 vs Claude-3 | GPT-4 vs Llama-3 | Claude-3 vs Llama-3 |
|--------|-------------------|------------------|---------------------|
| Pearson r | 0.92 | 0.89 | 0.87 |
| Agreement | Excellent | Excellent | Excellent |

**Overall Krippendorff's Œ±:** 0.84 (Excellent)

### Publisher Bias Rankings

| Rank | Publisher | Mean Bias | 95% Credible Interval | Classification |
|------|-----------|-----------|----------------------|----------------|
| 1 | Publisher C | -0.48 | [-0.62, -0.34] | Strong Liberal |
| 2 | Publisher A | -0.29 | [-0.41, -0.17] | Moderate Liberal |
| 3 | Publisher E | +0.02 | [-0.10, +0.14] | Neutral |
| 4 | Publisher B | +0.08 | [-0.04, +0.20] | Neutral |
| 5 | Publisher D | +0.38 | [+0.26, +0.50] | Moderate Conservative |

### Statistical Hypothesis Testing

**Friedman Test:**
- Test Statistic: œá¬≤ = 42.73
- P-value: < 0.001
- **Conclusion:** Statistically significant differences exist between publishers ‚úÖ

**Post-Hoc Pairwise Comparisons (Wilcoxon):**
- Publisher C vs Publisher D: p < 0.001 (Significant)
- Publisher C vs Publisher B: p = 0.003 (Significant)
- Publisher A vs Publisher D: p = 0.012 (Significant)
- Publisher E vs Others: Not significant (truly neutral)

---

## üéØ Key Findings

1. **Publisher Differences Are Real:** Statistically significant bias variations detected across publishers (Friedman p < 0.001)

2. **High Model Consensus:** Three frontier LLMs achieve 84% inter-rater reliability (Krippendorff's Œ± = 0.84)

3. **Quantified Uncertainty:** Bayesian framework provides 95% credible intervals for all publisher effects

4. **Neutral Publishers Exist:** Publisher E shows no credible bias (95% HDI includes zero)

5. **Within-Publisher Variability:** Individual textbooks vary around publisher means (œÉ_textbook = 0.23)

---

## üìä Visualizations

The notebook generates comprehensive visualizations:

- **Scatter Plots:** Pairwise LLM agreement (GPT-4 vs Claude-3, etc.)
- **Forest Plots:** Publisher effects with 95% credible intervals
- **Box Plots:** Bias distribution by publisher
- **Trace Plots:** MCMC convergence diagnostics
- **Violin Plots:** Textbook-level variability within publishers
- **Posterior Distributions:** Full probability densities for all parameters

---

## üíæ Production Deployment

### API Rate Limiting

```python
import time
from functools import wraps

def rate_limit(max_per_minute):
    """Decorator for API rate limiting"""
    min_interval = 60.0 / max_per_minute
    
    def decorator(func):
        last_called = [0.0]
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)
            result = func(*args, **kwargs)
            last_called[0] = time.time()
            return result
        
        return wrapper
    return decorator

@rate_limit(max_per_minute=60)  # 60 RPM limit
def query_gpt4(prompt: str) -> float:
    # API call logic
    pass
```

### Error Handling

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), 
       wait=wait_exponential(multiplier=1, min=4, max=10))
def robust_api_call(prompt: str, model: str) -> float:
    """Retry failed API calls with exponential backoff"""
    try:
        if model == 'gpt4':
            return query_gpt4(prompt)
        elif model == 'claude3':
            return query_claude3(prompt)
        elif model == 'llama3':
            return query_llama3(prompt)
    except Exception as e:
        logger.error(f"API call failed: {e}")
        raise
```

### Batch Processing

```python
from tqdm import tqdm

def process_textbooks(textbooks: List[Dict]) -> pd.DataFrame:
    """Process multiple textbooks with progress tracking"""
    results = []
    
    for textbook in tqdm(textbooks, desc="Processing textbooks"):
        for passage in textbook['passages']:
            ratings = ensemble.rate_passage(passage['text'])
            results.append({
                'publisher': textbook['publisher'],
                'textbook': textbook['id'],
                'passage': passage['id'],
                **ratings
            })
    
    return pd.DataFrame(results)
```

---

## üîÆ Future Enhancements

1. **Expanded LLM Ensemble:** Add Claude-4, Gemini Pro, Mistral-Large
2. **Fine-Tuning:** Train domain-specific bias detection models
3. **Explainability:** Add SHAP-like attribution for LLM decisions
4. **Real-Time Dashboard:** Streamlit/Gradio interface for interactive exploration
5. **Temporal Analysis:** Track bias evolution across textbook editions
6. **Multi-Dimensional Bias:** Extend beyond liberal-conservative to other axes
7. **MLOps Pipeline:** Airflow DAGs for continuous monitoring

---

## üìö References

### LLM APIs
1. OpenAI. (2024). GPT-4 Technical Report. https://platform.openai.com/docs
2. Anthropic. (2024). Claude 3 Model Card. https://docs.anthropic.com
3. Meta. (2024). Llama 3 Model Documentation. https://huggingface.co/meta-llama

### Statistical Methods
4. Krippendorff, K. (2011). Computing Krippendorff's Alpha-Reliability. *Departmental Papers (ASC)*.
5. Gelman, A., et al. (2013). *Bayesian Data Analysis* (3rd ed.). CRC Press.
6. Friedman, M. (1937). The use of ranks to avoid the assumption of normality. *JASA*, 32(200), 675-701.

### Bayesian Software
7. Salvatier, J., Wiecki, T. V., & Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. *PeerJ Computer Science*, 2, e55.
8. Kumar, R., et al. (2019). ArviZ: Exploratory analysis of Bayesian models. *JOSS*, 4(33), 1143.

### Educational Bias Literature
9. FitzGerald, J. (2009). Textbooks and politics: Policy approaches to textbooks. *IARTEM*.
10. Loewen, J. W. (2018). *Lies My Teacher Told Me: Everything Your American History Textbook Got Wrong*. The New Press.

---

## ü§ù Contributing

Contributions welcome! Areas for improvement:

- Add support for additional LLMs
- Implement alternative Bayesian priors
- Expand to multi-lingual textbooks
- Create web-based visualization dashboard
- Add automated testing suite

Please open an issue to discuss major changes before submitting a PR.

---

## üìß Contact

**Derek Lankeaux**
- GitHub: [@dereklankeaux](https://github.com/dereklankeaux)
- LinkedIn: [linkedin.com/in/dereklankeaux](https://linkedin.com/in/dereklankeaux)
- Email: derek.lankeaux@example.com

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- OpenAI, Anthropic, Meta for frontier LLM APIs
- PyMC development team for probabilistic programming framework
- ArviZ team for Bayesian diagnostics and visualization tools
- Rochester Institute of Technology Applied Statistics program
- Educational publishers for textbook access (with appropriate permissions)

---

## üìä Project Statistics

- **API Calls:** 67,500 (GPT-4, Claude-3, Llama-3)
- **Tokens Processed:** ~2.5 million
- **MCMC Samples:** 2,000 draws √ó 4 chains = 8,000 samples
- **Runtime:** ~12 hours (with rate limiting)
- **Code Quality:** Type hints, docstrings, error handling
- **Documentation:** Comprehensive notebook with markdown explanations

---

## üèÜ Impact

This framework has applications beyond textbook bias detection:

- **News Article Analysis:** Detect media outlet bias
- **Product Review Analysis:** Identify fake/biased reviews
- **Social Media Monitoring:** Track political discourse trends
- **Content Moderation:** Flag potentially biased content
- **Research Integrity:** Assess bias in scientific literature

---

**‚≠ê If this project helps your research or work, please consider citing it!**

```bibtex
@software{lankeaux2024textbook_bias,
  author = {Lankeaux, Derek},
  title = {Detecting Publisher Bias Using LLM Ensemble and Bayesian Methods},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/dereklankeaux/textbook-bias-detection}
}
```

*Last Updated: November 2024*
